## TF-IDF Baseline

- Recall@1: 0.347 (34.7%)
- Recall@3: 0.559 (55.9%)
- Recall@10: 0.745 (74.5%)
- MRR: 0.482 (48.2%)

Эти результаты говорят о следующем:
1. Модель правильно определяет релевантный документ как лучший результат примерно в 35% случаев
2. Правильный документ появляется в топ-3 результатах примерно в 56% случаев
3. Правильный документ находится в пределах топ-10 результатов примерно в 75% случаев
4. Средний обратный ранг (MRR) 0.482 указывает на то, что в среднем правильный документ появляется на 2-3 позиции в ранжированном списке

### Ограничения текущего подхода

1. **Семантическое понимание**: TF-IDF обрабатывает слова как независимые токены и не может улавливать семантические связи между словами. Он не понимает синонимы, перефразировки или контекстуальные значения.

2. **Порядок слов**: Подход игнорирует порядок слов в тексте, что может быть критически важно для понимания смысла вопросов и документов.

3. **Слова вне словаря**: TF-IDF не может обрабатывать слова, которых не было в обучающих данных, что может быть проблематично для специальной терминологии или новых слов.

4. **Нечувствительность к контексту**: Модель не может понимать более широкий контекст текста или конкретное намерение, стоящее за вопросами.

5. **Размерность**: Получающиеся векторы обычно имеют высокую размерность и разреженность, что может сделать процесс поиска вычислительно затратным и потенциально менее надежным.

6. **Языковая специфичность**: Подход требует языково-специфической предобработки и плохо справляется с многоязычным контентом.

## E5 Baseline

- Recall@1: 0.596 (59.6%)
- Recall@3: 0.798 (79.8%)
- Recall@10: 0.906 (90.6%)
- MRR: 0.710 (71.0%)

### Сравнение с TF-IDF

Сравнивая результаты E5 baseline с TF-IDF, можно отметить значительное улучшение по всем метрикам:

1. **Recall@1**: улучшение с 34.7% до 59.6% (рост на 24.9 процентных пункта)
2. **Recall@3**: улучшение с 55.9% до 79.8% (рост на 23.9 процентных пункта)
3. **Recall@10**: улучшение с 74.5% до 90.6% (рост на 16.1 процентных пункта)
4. **MRR**: улучшение с 48.2% до 71.0% (рост на 22.8 процентных пункта)

### Причины улучшения

1. **Семантическое понимание**: E5, будучи трансформерной моделью, способна улавливать семантические связи между словами и понимать контекст, что позволяет лучше сопоставлять вопросы с релевантными документами даже при использовании разных формулировок.

2. **Контекстуальная обработка**: Модель учитывает контекст каждого слова в предложении, что позволяет лучше понимать смысл как вопросов, так и документов.

3. **Предварительное обучение**: E5 была предварительно обучена на большом объеме многоязычных данных, что позволяет ей лучше работать с различными формулировками и языковыми конструкциями.

4. **Плотные эмбеддинги**: В отличие от разреженных TF-IDF векторов, E5 генерирует плотные эмбеддинги, которые лучше отражают семантическую близость текстов.

5. **Многоязычность**: E5 специально обучена для работы с несколькими языками, что делает её более универсальной и эффективной в обработке текста.

Эти улучшения демонстрируют преимущества современных нейронных подходов к поиску информации по сравнению с традиционными методами на основе TF-IDF.
